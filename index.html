<!-- Background Code - Split Screen -->
  <div class="background-box">
    <div class="code-scroll" style="width: 50%; left: 0;">
// TypeScript with Anthropic SDK
import Anthropic from "@anthropic-ai/sdk";

const client = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
});

async function analyzeCode(code: string): Promise<string> {
  const message = await client.messages.create({
    model: "claude-sonnet-4-20250514",
    max_tokens: 1024,
    messages: [
      {
        role: "user",
        content: `Analyze this code for security issues: ${code}`,
      },
    ],
  });
  
  return message.content[0].type === "text" 
    ? message.content[0].text 
    : "";
}

// Stream responses for real-time feedback
async function streamCompletion(prompt: string) {
  const stream = await client.messages.stream({
    model: "claude-sonnet-4-20250514",
    max_tokens: 1024,
    messages: [{ role: "user", content: prompt }],
  });

  for await (const chunk of stream) {
    if (chunk.type === "content_block_delta" && 
        chunk.delta.type === "text_delta") {
      process.stdout.write(chunk.delta.text);
    }
  }
}

// Multi-turn conversation handler
class ConversationManager {
  private history: Anthropic.MessageParam[] = [];

  async chat(userMessage: string): Promise<string> {
    this.history.push({
      role: "user",
      content: userMessage,
    });

    const response = await client.messages.create({
      model: "claude-sonnet-4-20250514",
      max_tokens: 2048,
      messages: this.history,
    });

    const assistantMessage = response.content[0].type === "text"
      ? response.content[0].text
      : "";

    this.history.push({
      role: "assistant",
      content: assistantMessage,
    });

    return assistantMessage;
  }
}

// Tool use example for code analysis
interface ToolDefinition {
  name: string;
  description: string;
  input_schema: {
    type: "object";
    properties: Record<string, unknown>;
    required: string[];
  };
}

const codeReviewTool: ToolDefinition = {
  name: "review_code",
  description: "Reviews code for best practices",
  input_schema: {
    type: "object",
    properties: {
      language: { type: "string" },
      code: { type: "string" },
    },
    required: ["language", "code"],
  },
};

async function reviewWithTools(code: string, lang: string) {
  const response = await client.messages.create({
    model: "claude-sonnet-4-20250514",
    max_tokens: 2048,
    tools: [codeReviewTool],
    messages: [
      {
        role: "user",
        content: `Review this ${lang} code: ${code}`,
      },
    ],
  });

  return response;
}

// Batch processing for multiple files
async function batchReview(files: Array<{name: string; content: string}>) {
  const reviews = await Promise.all(
    files.map(async (file) => {
      const result = await analyzeCode(file.content);
      return { file: file.name, review: result };
    })
  );
  return reviews;
}

// Error handling wrapper
async function safeCompletion(prompt: string): Promise<string> {
  try {
    const message = await client.messages.create({
      model: "claude-sonnet-4-20250514",
      max_tokens: 1024,
      messages: [{ role: "user", content: prompt }],
    });
    return message.content[0].type === "text" 
      ? message.content[0].text 
      : "No response";
  } catch (error) {
    if (error instanceof Anthropic.APIError) {
      console.error(`API Error: ${error.status} - ${error.message}`);
    }
    throw error;
  }
}
    </div>
    <div class="code-scroll" style="width: 50%; left: 50%;">
# Python with Anthropic SDK
import anthropic
import os
from typing import List, Dict, Generator

client = anthropic.Anthropic(
    api_key=os.environ.get("ANTHROPIC_API_KEY")
)

def analyze_security(code: str) -> str:
    """Analyze code for security vulnerabilities"""
    message = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=1024,
        messages=[
            {
                "role": "user",
                "content": f"Analyze security issues: {code}"
            }
        ]
    )
    return message.content[0].text

# Streaming for real-time responses
def stream_response(prompt: str) -> Generator[str, None, None]:
    """Stream Claude's response in real-time"""
    with client.messages.stream(
        model="claude-sonnet-4-20250514",
        max_tokens=1024,
        messages=[{"role": "user", "content": prompt}],
    ) as stream:
        for text in stream.text_stream:
            yield text

# Conversation management
class AIConversation:
    def __init__(self):
        self.history: List[Dict[str, str]] = []
    
    def chat(self, user_message: str) -> str:
        """Maintain conversation context"""
        self.history.append({
            "role": "user",
            "content": user_message
        })
        
        response = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=2048,
            messages=self.history
        )
        
        assistant_msg = response.content[0].text
        self.history.append({
            "role": "assistant",
            "content": assistant_msg
        })
        
        return assistant_msg

# Tool use for structured outputs
def review_with_tools(code: str, language: str):
    """Use tools for structured code review"""
    tools = [
        {
            "name": "code_analyzer",
            "description": "Analyzes code quality",
            "input_schema": {
                "type": "object",
                "properties": {
                    "language": {"type": "string"},
                    "code": {"type": "string"},
                    "severity": {"type": "string"}
                },
                "required": ["language", "code"]
            }
        }
    ]
    
    response = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=2048,
        tools=tools,
        messages=[
            {
                "role": "user",
                "content": f"Review {language}: {code}"
            }
        ]
    )
    
    return response

# Async batch processing
import asyncio

async def batch_analyze(files: List[Dict[str, str]]):
    """Process multiple files concurrently"""
    async def analyze_file(file_info):
        result = analyze_security(file_info["content"])
        return {"file": file_info["name"], "result": result}
    
    tasks = [analyze_file(f) for f in files]
    return await asyncio.gather(*tasks)

# Vision capabilities for diagrams
def analyze_architecture(image_path: str) -> str:
    """Analyze architecture diagrams"""
    with open(image_path, "rb") as img:
        image_data = base64.b64encode(img.read()).decode()
    
    message = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=1024,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image",
                        "source": {
                            "type": "base64",
                            "media_type": "image/png",
                            "data": image_data,
                        },
                    },
                    {
                        "type": "text",
                        "text": "Analyze this architecture"
                    }
                ],
            }
        ],
    )
    return message.content[0].text

# Error handling
def safe_completion(prompt: str) -> str:
    """Completion with error handling"""
    try:
        message = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=1024,
            messages=[{"role": "user", "content": prompt}]
        )
        return message.content[0].text
    except anthropic.APIError as e:
        print(f"API Error: {e.status_code} - {e.message}")
        raise

# Rate limiting helper
from time import sleep

def rate_limited_calls(prompts: List[str], delay: float = 1.0):
    """Make rate-limited API calls"""
    results = []
    for prompt in prompts:
        result = safe_completion(prompt)
        results.append(result)
        sleep(delay)
    return results
    </div>
  </div>
